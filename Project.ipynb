{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Activation, ZeroPadding2D, Add, Concatenate\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'celeba-dataset/img_align_celeba'\n",
    "attributes_path = 'celeba-dataset/list_attr_celeba.csv'\n",
    "image_folder = 'categories'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load attribute data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
       "       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
       "       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair',\n",
       "       'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee',\n",
       "       'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male',\n",
       "       'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
       "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
       "       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair',\n",
       "       'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
       "       'Wearing_Necklace', 'Wearing_Necktie', 'Young'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes_df = pd.read_csv(attributes_path)\n",
    "attributes_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_to_keep = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair', 'Male']\n",
    "attribute_data = attributes_df[attributes_to_keep]\n",
    "attribute_data = attribute_data.values # note that index starts with 0 and image id with 1!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_size = 100\n",
    "epochs = 75\n",
    "save_interval = 1\n",
    "batch_size = 512\n",
    "#batches_per_epoch = train_generator.n // batch_size\n",
    "train_generator= None\n",
    "data_generator = None\n",
    "#optimizer = Adam(0.0002, 0.5)\n",
    "optimizer = Adam(0.00002, 0.5)\n",
    "batches_per_epoch = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.000002, 0.5)\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true*y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_size = 100\n",
    "nbr_of_features = 5\n",
    "in_features = Input(shape=[nbr_of_features])\n",
    "noise_input = Input(shape=[noise_size])\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    generator_input = Concatenate()([noise_input, in_features])\n",
    "    #generator_input = Input(shape=[latent_dim + nbr_input_features])\n",
    "    fc = Dense(128*8*8)(generator_input)\n",
    "    #fc = Dense(16*16*3, activation='sigmoid')(generator_input)\n",
    "    fc = LeakyReLU(alpha=0.2)(fc)\n",
    "    image = Reshape((8,8,128))(fc)\n",
    "    \n",
    "    image = UpSampling2D()(image) # 16\n",
    "    image = Conv2D(128, kernel_size=3, padding=\"same\")(image)\n",
    "    image = BatchNormalization()(image)\n",
    "    image = LeakyReLU(alpha=0.2)(image)\n",
    "    \n",
    "    image = UpSampling2D()(image) # 32\n",
    "    image = Conv2D(64, kernel_size=3, padding=\"same\")(image)\n",
    "    image = BatchNormalization()(image)\n",
    "    image = LeakyReLU(alpha=0.2)(image)\n",
    "    \n",
    "    image = UpSampling2D()(image) # 64\n",
    "    image = Conv2D(32, kernel_size=3, padding=\"same\")(image)\n",
    "    image = BatchNormalization()(image)\n",
    "    image = LeakyReLU(alpha=0.2)(image)\n",
    "    \n",
    "    #image = UpSampling2D()(image) # 128\n",
    "    image = Conv2D(3, kernel_size=3, padding=\"same\", activation='sigmoid')(image)\n",
    "    \n",
    "    return Model([noise_input, in_features], image)\n",
    "\n",
    "def build_discriminator():\n",
    "    \n",
    "    image_input = Input(shape=(64,64,3))\n",
    "    \n",
    "    image = Conv2D(32, kernel_size=5, padding=\"same\", strides=2)(image_input) # 32\n",
    "    image = LeakyReLU(alpha=0.2)(image)\n",
    "    \n",
    "    image = Dropout(0.25)(image)\n",
    "    image = Conv2D(64, kernel_size=5, padding=\"same\", strides=2)(image) # 16\n",
    "    image = BatchNormalization()(image)\n",
    "    image = LeakyReLU(alpha=0.2)(image)\n",
    "    \n",
    "    image = Dropout(0.25)(image)\n",
    "    image = Conv2D(128, kernel_size=5, padding=\"same\", strides=2)(image)# 8\n",
    "    image = BatchNormalization()(image)\n",
    "    image = LeakyReLU(alpha=0.2)(image)\n",
    "    \n",
    "    image = Dropout(0.25)(image)\n",
    "    image = Conv2D(256, kernel_size=5, padding=\"same\", strides=1)(image) # 8\n",
    "    image = BatchNormalization()(image)\n",
    "    image = LeakyReLU(alpha=0.2)(image)\n",
    "    \n",
    "    image_flat = Flatten()(image)\n",
    "    \n",
    "    classifier_input = Concatenate()([image_flat, in_features])\n",
    "    \n",
    "    fc_size = 64\n",
    "    hidden = Dense(fc_size)(classifier_input)\n",
    "    hidden = LeakyReLU(alpha=0.2)(hidden)\n",
    "    out = Dense(1, activation='sigmoid')(hidden)\n",
    "    \n",
    "    return Model([image_input, in_features], out)\n",
    "#discriminator = Model([in_image, in_features], discriminator_classifier([current_discriminator_cnn(in_image), in_features]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(d_loss_hist, g_loss_hist):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(d_loss_hist, label='Discriminator')\n",
    "    plt.plot(g_loss_hist, label='Generator')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Loss over epochs')\n",
    "    \n",
    "feature_matrix = attribute_data[0:25,:]\n",
    "\n",
    "def numpy_rescaled(x):\n",
    "    out = x.copy()\n",
    "    out[x<0] = 0\n",
    "    out[x>1] = 1\n",
    "    return out\n",
    "\n",
    "def save_imgs(epoch, generator):\n",
    "    r, c = 5, 5\n",
    "    #noise = np.random.normal(0, 1, (r * c, noise_size))\n",
    "    #gen_imgs = generator.predict([noise, feature_matrix])\n",
    "    # 1: female, black hair\n",
    "    # 2: female, blonde hair\n",
    "    # 3: female, brown hair\n",
    "    # 4: male, grey hair\n",
    "    # 5: male, black hair\n",
    "    features = np.array([[1, -1, -1, -1, -1], [-1, 1, -1, -1, -1],[-1, -1, 1, -1, -1],[-1, -1, -1, 1, 1], [1, -1, -1, -1, 1]]);\n",
    "    # Rescale images 0 - 1\n",
    "    #gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        \n",
    "        for j in range(c):\n",
    "            noise = np.random.normal(0,1, (1, noise_size))\n",
    "            gen_img = generator.predict([noise, features[i].reshape(1,5)])\n",
    "            axs[i,j].imshow(gen_img[0,:,:,:])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"Images_wasserstein/final_no_wasserstein_%d.png\" % epoch)\n",
    "    plt.close()\n",
    "\n",
    "def train_gan(discriminator, generator, combined_model, epochs=10, batches_per_epoch=None):\n",
    "    if batches_per_epoch is None:\n",
    "        batches_per_epoch = train_generator.n // batch_size\n",
    "\n",
    "\n",
    "    d_loss_hist = list()\n",
    "    g_loss_hist = list()\n",
    "    for epoch in range(epochs):\n",
    "       \n",
    "        for i_batch in range(batches_per_epoch):\n",
    "        # Select a random batch of images\n",
    "            imgs, y = train_generator.next()\n",
    "            # one hot to feature\n",
    "            y = np.argmax(y, axis = 1)\n",
    "            input_features = attribute_data[y, :]\n",
    "            \n",
    "            b_size = imgs.shape[0]\n",
    "            valid = np.ones((b_size, 1))\n",
    "            fake = np.zeros((b_size, 1))\n",
    "\n",
    "            if i_batch%40 == 0:\n",
    "                print('Currently on batch: %d for epoch: %d' % (i_batch, epoch))\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (b_size, noise_size))\n",
    "            # Generator features\n",
    "            \n",
    "            gen_imgs = generator.predict([noise, input_features])\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = discriminator.train_on_batch([imgs, input_features], valid)\n",
    "            d_loss_fake = discriminator.train_on_batch([gen_imgs, input_features], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = combined_model.train_on_batch([noise, input_features], valid)\n",
    "\n",
    "        d_loss_hist.append(d_loss[0])\n",
    "        g_loss_hist.append(g_loss)\n",
    "        # Plot the progress\n",
    "        print (\"epoch: %d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch, generator)\n",
    "        if epoch % 10 == 0:\n",
    "            generator.save('generator_fine_tune_2_epoch_%d.h5' % epoch)\n",
    "            discriminator.save('discriminator_fine_tune_2_epoch_%d.h5' % epoch)\n",
    "    \n",
    "    return d_loss_hist, g_loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "# First generator step\n",
    "generator = build_generator()\n",
    "#current_generator_model.summary()\n",
    "\n",
    "# First Discriminator step, only classifier\n",
    "discriminator = build_discriminator()\n",
    "#discriminator_classifier.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "generator = load_model('generator_epoch_50.h5')\n",
    "discriminator = load_model('discriminator_epoch_50.h5')\n",
    "\n",
    "#save_imgs(-1, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_imgs(-1, generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_size = (64, 64)\n",
    "#input_shape = (target_size[0], target_size[1], 3)\n",
    "\n",
    "def get_data_generators(image_size):\n",
    "    data_generator = keras.preprocessing.image.ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "    train_generator = data_generator.flow_from_directory(image_folder, \n",
    "                                                         target_size=(image_size, image_size), \n",
    "                                                         batch_size=batch_size)\n",
    "    return data_generator, train_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss='binary_crossentropy', \n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "img = generator([noise_input, in_features])\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator([img, in_features])\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model([noise_input, in_features], valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 images belonging to 202599 classes.\n"
     ]
    }
   ],
   "source": [
    "data_generator, train_generator = get_data_generators(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on batch: 0 for epoch: 0\n",
      "epoch: 0 [D loss: 0.408586, acc.: 81.64%] [G loss: 0.015929]\n",
      "Currently on batch: 0 for epoch: 1\n",
      "epoch: 1 [D loss: 1.122392, acc.: 58.11%] [G loss: 0.008966]\n",
      "Currently on batch: 0 for epoch: 2\n",
      "epoch: 2 [D loss: 2.006832, acc.: 50.29%] [G loss: 0.007024]\n",
      "Currently on batch: 0 for epoch: 3\n",
      "epoch: 3 [D loss: 2.529087, acc.: 49.32%] [G loss: 0.006569]\n",
      "Currently on batch: 0 for epoch: 4\n",
      "epoch: 4 [D loss: 2.836002, acc.: 50.10%] [G loss: 0.003931]\n",
      "Currently on batch: 0 for epoch: 5\n",
      "epoch: 5 [D loss: 3.350276, acc.: 49.51%] [G loss: 0.002307]\n",
      "Currently on batch: 0 for epoch: 6\n",
      "epoch: 6 [D loss: 3.327423, acc.: 49.22%] [G loss: 0.002714]\n",
      "Currently on batch: 0 for epoch: 7\n",
      "epoch: 7 [D loss: 3.623621, acc.: 49.80%] [G loss: 0.002403]\n",
      "Currently on batch: 0 for epoch: 8\n",
      "epoch: 8 [D loss: 3.703610, acc.: 49.61%] [G loss: 0.002207]\n",
      "Currently on batch: 0 for epoch: 9\n",
      "epoch: 9 [D loss: 3.754653, acc.: 49.71%] [G loss: 0.001253]\n",
      "Currently on batch: 0 for epoch: 10\n",
      "epoch: 10 [D loss: 3.735864, acc.: 49.80%] [G loss: 0.001565]\n",
      "Currently on batch: 0 for epoch: 11\n",
      "epoch: 11 [D loss: 3.966875, acc.: 49.80%] [G loss: 0.001555]\n",
      "Currently on batch: 0 for epoch: 12\n",
      "epoch: 12 [D loss: 3.932774, acc.: 49.80%] [G loss: 0.001053]\n",
      "Currently on batch: 0 for epoch: 13\n",
      "epoch: 13 [D loss: 4.327984, acc.: 49.41%] [G loss: 0.000646]\n",
      "Currently on batch: 0 for epoch: 14\n",
      "epoch: 14 [D loss: 4.141020, acc.: 49.80%] [G loss: 0.001238]\n",
      "Currently on batch: 0 for epoch: 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-98333bbd3067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md_loss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-3e64fab1aa28>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(discriminator, generator, combined_model, epochs, batches_per_epoch)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Select a random batch of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;31m# one hot to feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1817\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m   1770\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m                            interpolation=self.interpolation)\n\u001b[0m\u001b[1;32m   1773\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    515\u001b[0m                         \", \".join(_PIL_INTERPOLATION_METHODS.keys())))\n\u001b[1;32m    516\u001b[0m             \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PIL_INTERPOLATION_METHODS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth_height_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1761\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_loss_hist, g_loss_hist = train_gan(discriminator, generator, combined, epochs=epochs, batches_per_epoch=batches_per_epoch)\n",
    "plot_history(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the models from step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('generator_finalefinetune2.h5')\n",
    "discriminator.save('discriminator_finalefinetune2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
